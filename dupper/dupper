#!/usr/bin/perl -w
#
# $Id$
#
# The author disclaims all copyrights and releases this script into the
# public domain.
#
# Detects and deals with duplicate files.
#
# Run perldoc(1) on this script for additional documentation.

use strict;

use Digest::SHA1 ();
use File::Find qw(find);
use File::Spec ();

# we reset this for each file to avoid OO overhead on each file
my $digest = Digest::SHA1->new;

use Getopt::Std;
my %opts;
getopts 'h?', \%opts;

print_help() if $opts{h} or $opts{'?'};

my %defaults;
$defaults{bydepth} = 0;

# read from STDIN if no args left
chomp( @ARGV = <STDIN> ) unless @ARGV;

# only deal with regular files or directories
@ARGV = grep { -d $_ || -f _ } @ARGV;
print_help() unless @ARGV;

my ( %seen, %duplicates, @empty );

find( { wanted => \&find_dups, no_chdir => 1, bydepth => $defaults{bydepth} },
  @ARGV );

for my $key (keys %duplicates) {
  next if @{ $duplicates{$key} } < 2;
  print "dups: @{ $duplicates{$key} }\n";
}

sub find_dups {
  return unless -f;

  my $size = -s;

  # empty files are a special case: duplicates make no sense for them
  if ( $size == 0 ) {
    push @empty, $_;
    return;
  }

  unless ( exists $seen{$size} ) {
    $seen{$size}->{firsthit} = $_;
    return;
  }

  # generate checksum on first file hit of this size, as now have
  # second of same size
  if ( exists $seen{$size}->{firsthit} ) {
    my $checksum = gen_checksum( $seen{$size}->{firsthit} );
    push @{ $duplicates{$checksum} }, $seen{$size}->{firsthit}
     if defined $checksum;

    delete $seen{$size}->{firsthit};
  }

  my $checksum = gen_checksum($_);
  push @{ $duplicates{$checksum} }, $_ if defined $checksum;
}

sub gen_checksum {
  my $filename = shift;

  unless ( open FILE, "< $filename" ) {
    # TODO warn about permissions problems or whatnot
    return;
  }

  $digest->addfile(*FILE);
  close FILE;

  return $digest->b64digest;
}

# a generic help blarb
sub print_help {
  print <<"HELP";
Usage: $0 [options] [file-or-directory ..]

Detects and deals with duplicate files.

Options:
  -h/-?  Display this message

Run perldoc(1) on this script for additional documentation.

HELP
  exit 100;
}

=head1 NAME

dupper - detects and deals with duplicate files

=head1 SYNOPSIS

To get a list of duplicate files in a particular directory:

  $ dupper ~/public_html/images

=head1 DESCRIPTION

=head2 Overview

Finds duplicate files in one or more input files and directories.
Duplicates are matched though the use of SHA1 checksums; files are pre-
indexed by size for speed: checksums are only done when multiple files
share the same file size.

Handling for duplicates has yet to be written in this release.

=head2 Normal Usage

  $ dupper [options] [file-or-directory ..]

See L<"OPTIONS"> for details on the command line switches supported.

A list of files and directories to operate on should be specified on the
command line. Failing that, the script will attempt to read data from
standard input.

=head1 OPTIONS

This script currently supports the following command line switches:

=over 4

=item B<-h>, B<-?>

Prints a brief usage note about the script.

=back

=head1 BUGS

=head2 Reporting Bugs

Newer versions of this script may be available from:

http://sial.org/code/perl/

If the bug is in the latest version, send a report to the author.
Patches that fix problems or add new features are welcome.

=head1 SEE ALSO

perl(1)

=head1 AUTHOR

Jeremy Mates, http://sial.org/contact/

Pre-index on size concept adapted from code by Craig Reyenga.

=head1 COPYRIGHT

The author disclaims all copyrights and releases this script into the
public domain.

=head1 VERSION

  $Id$

=head1 SCRIPT CATEGORIES

Utilities

=cut
