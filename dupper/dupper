#!/usr/bin/perl -w
#
# $Id$
#
# The author disclaims all copyrights and releases this script into the
# public domain.
#
# Detects and deals with duplicate files.
#
# Run perldoc(1) on this script for additional documentation.

use strict;

use Digest::SHA1 ();
use File::Find qw(find);
use File::Spec ();

# we reset this for each file to avoid OO overhead on each file
my $digest = Digest::SHA1->new;

use Getopt::Std;
my %opts;
getopts 'h?', \%opts;

print_help() if $opts{h} or $opts{'?'};

my %defaults;
$defaults{bydepth} = 0;

# read from STDIN if no args left
chomp( @ARGV = <STDIN> ) unless @ARGV;

# only deal with regular files or directories
@ARGV = grep { -d $_ || -f _ } @ARGV;
print_help() unless @ARGV;

my ( %seen, %duplicates );

find( { wanted => \&wanted, no_chdir => 1, bydepth => $defaults{bydepth} },
  @ARGV );

use Data::Dumper;
warn Dumper \%seen, \%duplicates;

sub wanted {
  return unless -f;

  my $size = -s;

  # TODO need means to control handling of empty files
  #return if $size == 0;

  # TODO need to go back over any existing files in size index and
  # calculate checksums on them now that have size hit... but only if
  # there is only a single file, as otherwise will have done a checksum?
  if ( exists $seen{$size} ) {
    unless ( open FILE, "< $_" ) {
      # TODO handle permissions problems or whatnot

      return;
    }

    $digest->addfile(*FILE);
    my $checksum = $digest->b64digest;
    $seen{$size}->{dups}->{$checksum}++;

    if ( $seen{$size}->{dups}->{$checksum} > 1 ) {
      warn "duplicate: $_\n";
      push @{ $duplicates{$checksum} }, $_;
    }
  }

  push @{ $seen{$size}->{files} }, $_;
}

# a generic help blarb
sub print_help {
  print <<"HELP";
Usage: $0 [options] [files]

Detects and deals with duplicate files.

Options:
  -h/-?  Display this message

Run perldoc(1) on this script for additional documentation.

HELP
  exit 100;
}

=head1 NAME

dupper - detects and deals with duplicate files

=head1 SYNOPSIS

To get a list of duplicate files in a particular directory:

  $ dupper ~/public_html/images

=head1 DESCRIPTION

=head2 Overview

Script to find (and optionally remove) duplicate files in one or more
directories. Duplicates are spotted though the use of SHA1 checksums.

=head2 Normal Usage

  $ dupper [options] [dir1 dir2 .. dirN]

See L<"OPTIONS"> for details on the command line switches supported.

A list of directories to operate on should be specified on the command
line. Failing that, the script will attempt to read directories from
standard input.

=head1 OPTIONS

This script currently supports the following command line switches:

=over 4

=item B<-h>, B<-?>

Prints a brief usage note about the script.

=item B<-v>

Verbose mode, a little bit more chatty.

=item B<-q>

Quiet mode, overrides verbose mode, turns off reporting.

=item B<-u>

Attempt to unlink any duplicates past the first. Default is just to
report the duplicate files. Files are added least-depth first by order
of sort(). In other words, be sure you are deleting the right thing.

=item B<-l>

Local-only mode; script will not recurse into subdirectories of any
directories passed to the script.

=item B<-g>

Make checksums apply across all directories on the command line. Default
is to treat the various directories supplied to the program as their own
separate realms.

=item B<-z>

Overrides B<-g>. Limits the scope of checksums to only other files in
the exact same local directory as one another. Much tighter than the
default scope.

=item B<-s> I<expression>

Perl expression that will result in the current file (stored in $_)
being skipped if the expression turns out to be "true." Example:

  -s 'm/^\\.rsrc\$/ || -z \$pti || -s \$pti > 1048576'

Would skip the checksum on files named '.rsrc', or files that are empty
via the B<-z> is-empty test, or files larger than a megabyte.

=item B<-p> I<expression>

Perl expression that will result in the current directory (stored in $_)
being pruned out of the tree. Like config dirs, for example:

  -p 'm/etc/'

Both B<-s> and B<-p> have access to the filename in $_, and can find the
full filepath in the variable $pti. (Short for "path to item" in case
you were wondering.)

=back

=head1 EXAMPLES

To remove duplicate files under a web area via cron, skipping html
documents, matching only in local directories, and being very quiet:

  21 6 * * * dupper -uqzs 'm/\.html$/' /www/example/images

=head1 BUGS

=head2 Reporting Bugs

Newer versions of this script may be available from:

http://sial.org/code/perl/

If the bug is in the latest version, send a report to the author.
Patches that fix problems or add new features are welcome.

=head2 Known Issues

Need to first index files by size somehow, so can speed the SHA1
operations to just cases where the files are of identical sizes: no
files of different size will ever be the same.

=head1 TODO

Change out the hackish manual directory recurser with
L<File::Find|File::Find>.

=head1 SEE ALSO

perl(1)

=head1 AUTHOR

Jeremy Mates, http://sial.org/contact/

=head1 COPYRIGHT

The author disclaims all copyrights and releases this script into the
public domain.

=head1 VERSION

  $Id$

=head1 SCRIPT CATEGORIES

Utilities

=cut
